 Sorry for the delay, so let's start the second lecture for our GPU101 course. I hope that you can hear me, or online you can hear me. If I'm not answering in the chat, please open the mic and ask the question. Okay, so I confirmed the schedule that I presented to you on Monday. So the submission for the project will be by the end of January and then the presentation after the course starts, okay, in the second semester so that you can have a little bit more time to prepare the presentation and we can be a bit more relaxed. I will present you and assign you the project by the end of the month. I will try to do that as soon as possible. Okay. So that you can start right away writing your project. I will assign you an algorithm. You will have the C baseline. You will have to optimize the code and run it on GPU. I'm not interested in like having the best performance out of the state of the art of that algorithm, but I'm interested in seeing you building up an implementation and optimizing and explain to me why you chose to optimize the algorithm in that way. OK, so why did you implement that specific portion in a certain way? Why did you choose to use a certain type of memory? So explain to me everything that you chose and why, most importantly. I also report here in the slides again all the ways that you have to access a GPU. So if you have a GPU you can take two routes depending on whether you have Windows or Linux. Otherwise you can always resort to Google Column. If you have any trouble with that, write me an email and maybe I can come 10 minutes earlier or I'll be turning in order to help you in the setup. The course schedule, I'm sorry for telling you today the room change, but it was next to the expected one. there shouldn't be any other change and from now on all the lectures will be on Thursday. So remember that. The WebEx room is still the same and you will have all the material in the OneDrive folder. So, running back from what we saw on Monday. So we started our introduction by saying, okay, we need more performance, how we can bridge the gap between the performance that the processor we have now can deliver and what we want in order to get our real-time processing. So we saw that GPUs are one of the best platforms in order to do that, in order to achieve our goal. this such a nice framework that is called CUDA that allows us to program NVIDIA GPUs. So what is CUDA? CUDA is a platform that is designed jointly at the software and web level in order to enable general-purpose computing based on GPUs. CUDA has three levels, so at the software it allows to program GPUs at the firmware, it offers a driver that is dedicated to NVIDIA GPUs and at the hardware, we have could allow us to expose parallelism, GPU parallelism that is designed for general post computing via a number of multi-processor endowed with cores and memory hierarchy. So from an application perspective we saw that we have an architecture model defined by CUDA, a programming model but also a memory model. So what we saw last time was the programming model. Programming model is based on the fact that we have threads that are computing the same instructions in parallel, threads are grouped into blocks and then we have blocks that are organized within grids. Okay? Everyone okay with that? Do we have some doubts or need some clarification on that? If not, we can go on. And in today's lecture we are going to see a bit more on the programming model, on the fact that we may need some kind of special patterns in order to attain, to expose parallelism and attain performance out of GPUs and then we will move to the memory model. So we saw that in order to efficiently parallelize, efficiently compute an algorithm with GPUs, we need to expose parallelism. And we said that there is this very nice model, there are two models, one is the SPMD, so single program multiple data and the SIMT, single instruction multiple threads, where threads execute the same instruction at the same time. OK, we said that, OK, what is done in the SIMT model, we assign a single instruction that is performed by multiple threads, each thread is going to compute its own element. So, we need to map the data to the threads. How can we do that? This process is called data decomposition. It's adopted to define the kernel function and we divide our data across the threads and we can do two things. We have two options. We can map the input data to the threads so that each thread is going to compute some kind of elaboration on the assigned input data. Or we can do the other way around, that is, we map the thread to the output data. data. We will see some examples and we have these two ways we can pursue in order to expose parallelism and extract the performance that we need. So starting from output data decomposition. Output data decomposition means the trite is mapped to the single output data item. So we can have two scenarios, one-to-one and one function. So let's see the two examples that we have here. The first one is the matrix multiplication. So we need that in order to compute a single item in the result matrix, we need to multiply a row and a column, okay? And some of the values, all right? So what can we do in this case is assign either, we can assign one thread to each element of the output matrix and each thread is going to compute the multiplication that we need from the rows and the columns. Okay? So in this case we have the single element that is using the multiple data in order to compute the single element. We can map it to many to one if we want. Another approach is another example that we can see is the RGB to grey conversion. So we know that images are matrices and each element of the matrix is a pixel, each pixel is defined by the three colour components that are green, red and blue. So what we need to do to convert the three pixel values to grain, we need to do some kind of multiply-add, let's say. So we can map each thread to the output pixel that we want to compute, so to identify the level of grain, And then each thread is going to take the corresponding point in the input matrix and convert the value from RGB to gray. We have a one-to-one correspondence between the position of the output element with the position of the input element. These are two examples where we can apply output data decomposition. The other approach is input data decomposition. In input data decomposition, we map the thread to the input data. Each thread is going to compute, to analyze a single element in the input to provide an output. In this case, for example, we can consider an example of input data decomposition, which is the computation of the histogram. The histogram is where we count the occurrences of, in this case, the letters within a text. So each time we encounter an h, we increase the counter for the h's. So what we do here is, in this case on GPU, each thread is going to analyze a single character and increase the counter relative to that specific character. We will see how can we implement this in a fast way and parallelize the computation because as you may notice, we might have some kind of problem since this is a very sequential task. So we will see next time the implementation on the instrument. Then, besides the mapping of the data that we might have between threads and the data, another problem that we have to face is the selection of the dimensionality of the grid. As we said, we have three dimensions that we have to properly set. It becomes natural to map one-dimensional problems to grids that have only one dimension. Matrices most likely will be analyzed with 2D grids, so that each block is going to analyze a portion of the matrix, a 2D portion of the matrix. And for 3D volumes, we might have 3D grids. Note that most of the times you can map higher dimensionality problems to lower dimensional ones. So, for example, matrices we can map them to 1D problems if we linearize the data. by taking into account that we are linearizing a 2D structure, but we could do that, depending on the application that we are considering. This becomes even more relevant considering that even if we are choosing a three-dimensional grid, the gritty is linearized internally by the compiler. Okay? And everything is done with a row-major layout. So what does this mean? If we have a matrix, okay, we will have all the rows of the matrix one next to each other. Okay? Everything good so far? For the cubes, we'll have all the rows of a slice next to each other, then the second slice, then the third. Okay? Everything good? Stop me if you have any questions. Another important thing to consider is that not only the grid is linearized, but also CUDA does not support the multidimensional allocation. Everything is linearized. So, for example, what we have here in the OS code, we have a matrix. With the CUDA malloc we have to allocate the device memory for that input matrix. What we are going to do? We are going to linearize the matrix. What is being actually done is that it is going to be allocated a very long array whose number of elements is going to be the same as the number of elements that we have in our matrix. So, the number of elements is rows times columns multiplied by the bytes that we need. We invoke the kernel, this is going to multiply all the elements by 2 and we have a, most likely this is a bidimensional grid. not reported here but we can notice that we have an index, okay this is an index, i is an index, that is related to the x components of our grid. We have thread ID x, x block dim along x and block ID x on x. And j instead is related to the y components. So we have thread IDX Y, block DIN Y and block IDX Y. So this is most likely a bidimensional grid and what we are doing is mapping each element of the input and output matrix because it's the same. we are overwriting the data. What we are doing is multiplying each element by 2. So how do we find the elements? We have to linearize the matrix. Once it's linearized, then we can use these components in order to access each element. So let's say for example we want to compute the element that is in . Element , most likely, will be computed by the block with index 0 and since it is in the upper left corner, it's going to have an index on x that is 0 and on y that is 0, so imagine that the axis goes down and on to the right. So this is going to be zero and then we have zero. The problem is that if we are considering block zero and then we are considering the upper left corner, everything sounds good because we have only zero elements. But then, the first block is going to compute the one next to the one that block 0 is processing. We need to shift the data, we need to shift the indices in order to map the thread with index 0, 0 to the first element that we have in the second matrix that we are considering. So this is done by considering the block dimension. So we know that each thread in each block is going to compute a certain number of elements that will be along the x-axis will be blocked along x. And blocked along y will give us the number of threads that we have on the y-dimension. Multiplying these two values we are going to have the complete number of elements that our block is processing. So, in order to get the number of elements that we need to shift in the first block, we are going to consider the block dimension. So the block dimension is going to give us the number of elements to skip. And multiplying it by the block index is going to provide us with the exact shift with respect to the beginning of the matrix. I hope everything is ok. We have the two indices, so we have them like 0,3, 0,4 or for example 7,3. These are all relative to the upper left corner, let's say, or they are all relative, they are not linear, so we are considering two coordinates within a matrix. We need to linearize, so we need to identify the row and then the shift with the column. And this is done the same way here and then we multiply by 2, that is our other input. important things to notice here in a raised axis we are going to multiply the number of columns to with the with the with the we are going to identify to multiply the number of column times j in order to identify the row and then plus i allows us to identify the column And here we have 1D memory allocation. Of course, in this case we use a row-measure layout, so we have one row next to the other. Sometimes a column-measure can be used, it might provide us with more performance than a row-measure layout. depends on the case. Another problem that we have when we have to select the grid dimensionality is what do we do when the data size is not a multiple of the number of the trends in the grid? We know that it is best to provide blocks with a number of threads that is a multiple of 32. Because 32 is the size of the warp in NVIDIA GPUs. So we know that those 32 threads are going to compute the same instruction at the same time and the pattern is optimal. Imagine that we have 16 x 16 blocks. What do we do when the blocks are bigger than the data that we have? It was something that came up also in the last lecture, when we said what do we do if we have an array that is 34 items and we have to schedule more than 32 threads. We have to check what we need to do is we need to check that the size, that the item we want to process is a valid item. So let's take the example that we did last time. So we have a 34 element array and we want to compute it with a 64 thread block. What we need to do is to check that each thread is going to compute a valid element. So basically we need to check if the ID of the thread is below the size of the entire array that we want to process. So if I'm thread number 5, I'm going to be assigned to element in the array that is at the index of 5 that works because that is a valid element. But if I'm, I don't know, if I'm thread like, we said like 64 threads, okay I'm thread 50, I'm not assigned a valid element. So I need to pause my execution, I need to remain idle. I do not need to compute anything, ok? Because I'm not assigned a valid element, so the pointer that might be assigned to me, ok, does not refer to a valid area in memory. So I remain idle. That is basically what is done here. So this is the same vector addition that we saw last time. This identifies the index of the assigned element for this thread. And we need to check if that thread is assigned something valid. This is valid for threads that have an index below dim, that are assigned an element below dim, otherwise that thread will do nothing. The specific term is that the thread remains idle. I D L E. So block size should be much more than 32 for performance reasons, but the block size sometimes is not a divider of the data size. you need to do is to round up. So we saw that for example we had 64 threads before, we rounded up the number of blocks to the first greater integer value. And the last thread in the the last block does nothing. Okay? So, GPU, this is still on the grid dimensionality on the block size or the number of threads that we have to scale. These are more like general rules that you might want to follow in order to entune your system. The other thing you can do is like brute forcing all the possible combinations and then get the best configuration. Sometimes you need to do that. But these are just like rule of thumb that you can follow in order to identify maybe not the best configuration but but one that might give you a nice performance. Okay? GPUs are throughput-oriented systems, so we get the best performance when we fully use the GPU. Okay? We do not want to have a low occupancy, because this means we are wasting resources. We are not using the GPU at its full power, So we are wasting resources that are precious in computing. Okay? So high resource utilization is obtained by interleaving many warps in the streaming multiprocessor. This allows us to hide the latency of operations that require a lot of time to execute. For example, we talked about that last time. We have memory accesses, especially to device memory. These are very costly. OK, we will see that in a bit more detail later. These operations are very costly. So what does the GPU do to overcome this issue? Schedules another instruction while waiting for the data to come and to get to the data that needs that data. So the GPU is able to perform a lot of context switches in order to hide the latency. The occupancy must be maximized. and we have a nice formula to compute occupancy, but still we need to perform a lot of profiling in order to fine tune our system and get the best performance. Also, occupancy is constrained. We talked about this also last time. registers are the fastest type of memory that we have, but they are scarce. And besides being scarce, there is a fixed number of registers that can be assigned to a block. So you cannot go beyond that limit. And also requiring many registers will increase the amount of resources that a block requires. And since the number of registers on a streaming multiprocessor is fixed, this means that having more registers per block, we can have fewer blocks per streaming multiprocessor. There is a compiler flag to get the number of registers but also to set it to a maximum. Another key factor is the amount of shared memory, that is the programmable cache that we have in the GPU. We will see that very in detail later. Again, this should be a recap of some rule of thumb that we have to tune the grid size. So the block size should be a multiple of the warp size. So anything that is a multiple of 32 is good. small block sizes because as we said last time if I schedule 16 threads then the GPU will schedule still 32 but I'm only using 16. This is a waste. You will have to tune the block size up and down according to the resource requirements of the kernel and the number of blocks should be much larger than the number of streaming multiprocessors, because each streaming multiprocessor can have up to 32 blocks. OK, so imagine that there are latest GPUs that have more than 100 streaming multiprocessors, then having 32 blocks for each streaming multiprocessor will give us a great number of blocks that is going to compute stuff in parallel. And this is a nice way to exploit parallelism and exploit the GPU fully. OK. And then, as I said before, you can do systematic experiments in order to get the best configuration. OK. This might require a lot of testing, but sometimes it's worth it. OK. This is still on the block size, related to the block size, kind of. No, this is not on the block size, okay. So, another thing that we have to avoid in our code is warp divergence. So, warps, we know that they are computing the same instruction at the same time, But if we make our work take a branch, this is going to introduce delay in our computation, because branches are serialized, interleaved maybe by the compiler, because we are not only by the compiler, but also by the scheduler, that is going to interleave instructions from the two branches, but still the time is longer. they are not executing parallel. Okay? So, to avoid branch diverges, we have to find some mechanism in order to prevent that. For example, in this case, what do we have? We have a branch that tells us if this is an even thread, not another thread, then do something, otherwise do something else. So we need to find sometimes ways to modify our code in order to map, to have this mapping, in order to have threads that are not idle, or at least minimize the number of threads that are idle. Okay? Then again before... how many slides do we have? Okay, I have a few slides. Before moving to the the Coda memory model. This is something we talked about last time, so we are going to see in more detail some things that we saw last time. Blocking and non-blocking API. So we have some APIs that are blocking and non-blocking. Blocking and non-blocking means that the host application, the CPU application, is going to wait or not for the completion of the operation. Each operation that involves the GPU is enqueued to a software queue that is called stream. So we have a series of operations that are sent to the GPU and the GPU executes them in order. It is a software queue. We have some APIs, some functions that require the host application to block its execution and wait for the completion of that task. For example, Kodam and Copies are blocking APIs. That means that once we copy the data, the host application does not proceed with its own execution until the copy is completed. because, for example, man copies involves data that is managed by the host. Right. There are also non-blocking kernel functions, non-blocking functions that are, for example, the kernel launch that do not involve, for example, the host. The kernel is pretty much GPU only execution. OK, we only have the launch that depends on the host application. This means that since the GPU has its own scheduler, its own data, its own memory, it is a complete separate system from the CPU. So the CPU can continue its execution while the GPU computes its own data. But if we want to get the results from the GPU, we need to wait for the results and then get the results from the memory. So we need some kind of barriers in order to synchronize the execution of the host and the kernel. Remember that sometimes it might be useful to have something that overlaps with the kernel. For example, we have an application that requires some kind of preprocessing before going to the GPU. And maybe the preprocessing and the offloading to the GPU is encapsulated in a for loop. So we are doing reprocessing, kernel, synchronization, reprocessing, kernel, synchronization, on different data. This is sequential, so we have this alternating pattern. But considering that we have no blocking calls to the kernel, we can do something more and reduce the entire time that we require to process the entire data. We can overlap the preprocessing with the kernel execution and wait for the GPU once we finish the preprocessing. the preprocess is only taken and or by the host. So after the host launches the kernel, the GPU starts its own computation. The CPU can start preprocessing some stuff, then get the result and schedule the second batch of data to the GPU. In this way, we can kind of pipeline the kernel launch and the preprocessing, overlap the execution, and reduce the entire computation time. This is something that is being done, this is not some kind of strange stuff, but we will see also this, this is the basic idea that we have at the basis when we use streams, CUDA streams. Because we have streams that are sequence of commands, but we can use them in a fancier way than this, in order to overlap host preprocessing, memory transfers and kernel execution. We will see that later. So we need some kind of synchronization. At a certain point we need that. So we have barriers that are synchronization points where all the threads have to stop and cannot proceed with the computation. Could a device synchronize is an AVI that is used to synchronize the host execution. We do not have only this type of barrier. We might have some kind of barrier that is needed within the kernel. The threads, maybe we have a branch, and there are some threads that are idle, and some that are executing the branch body. So we need a barrier that says, okay, stop at this point, do not go on with the computation, because we have to wait for the other trends. If we are parallelizing the computation, we might need, if the computation, the pattern is not uniform, we need a synchronization point in order to be sure to get the correct result. Also, trends cannot share data without synchronization. So if we need some part of the computation that is shared among the threads, for example, after an if branch, then we need a synchronization point that allows us to exchange the data across the threads. We can have synchronization at the block level or at the grid level. The grid level was the could a device synchronize. It was like an even broader level, but let's say it is at a grid level. And within the block we have another instruction that is dedicated to thread synchronization. By calling this function, synchronizeThreads, we can synchronize and put a barrier to the threads. So this is a graphic example. So we have threads that finish their execution in different time instances. But we need for all the threads to complete their operation before going on. So we put a barrier, that means all the threads, especially those that finish early, have to wait for the completion of the operation for all the others. In this case, N-2 is the one that is taking the longest, so all the other threads have to wait for this thread to complete. and then they can go on for the remaining part of the computation. Grid-level synchronization, instead, there is no actually a specific barrier that we can call. We either need to split the kernel or to invoke two-kernel in sequence in order to have this type of synchronization. Actually, this is not true in the latest revisions of Huda, because they introduced a concept that is called cooperative groups, that allows to get grid-level synchronization from within the kernel. Usually what is being done is either I split the kernel or invoke two kernels. Either I split or I invoke two kernels one next to the other. If you use latest versions of Huda and latest GPUs, then you can use cooperative groups that allows you to get grid level synchronization from within the kernel. you get the object that in the computation identifies the grid and then there is a sync API on that object that synchronizes the entire grid. But this is the classical way to do it. I'm not sure we saw this the last time, the timers, how to get the time. The fastest way to measure performance is to get the time of the data, of the computation. So we can do that by using CPU or GPU timers. With CPU timers, remember that we need a blocking call before, that is after the device is synchronized, that ensures us that the kernel computation is complete. So we can use for example either this function that is the classical get time of day, we You can use more libraries such as Chrono if you are writing C++ code, for example. But this means get a timestamp before the execution of the kernel, launch the kernel, synchronize and then get the timestamp for the completion of the synchronization operation. That allows you to get the entire kernel execution time. Another approach consists of using code events. Code events are markers that are pushed together with the other commands to the streams. So basically what we have here, we create events, we record the fact that we have an event at this point, the kernel execution and then we have the other event. Since these two events are related to the GPU and pushed into the stream queue that is dedicated to the GPU, these are going to measure the execution time. They are GPU related operations. So those are going to be recorded before and after the kernel completes. Otherwise, if you are not sure and you don't want to use events, you can still use the CPU finers. That works just fine. Okay? Still, remember, you have to synchronize. We kind of completed the programming model, we saw the architectural model. Let's move finally to the memory model. Memory model allows the user to exploit all the memories that we have in the GPU. The CUDA memory model makes the types of memory not transparent to the user. There are some types of memory that are on the GPU that we can manage explicitly using include APIs and this allows us to tune resources, maximize occupancy and maximize utilization. Everything good so far? We can move on to the... okay, yes, let's go. Quick recap on the memory hierarchy, hardware memory hierarchy. So this is what is actually on the silicon. We have register, caches and device memory. The names are important because sometimes we can and mix them up. Some names are the same in the hardware memory hierarchy and in the CUDA memory model, because we still have registers. Beware that device memory is much larger than global memory. We will see that later. Remember that we have specific names that are going to identify specific things. So first we have registers. Private. There is a private register file within each streaming multiprocessor. They are partitioned across the threads that we have running on the specific streaming multiprocessor. and in most cases are not enough to store all the private data for each thread. This means it is important to make a considerate use of registers. Remember that we have some types of resources such as registers that are not very abundant on the GPU. This means also that when we do not have abundant resources they are usually faster. And those are the ones that we may want to use when we compute our application. So these are closer to trend, this means they are very fast, but in turn they are scarce. So we have to find a trade-off between these two things. We have resistance, we have the device memory that is off-chip, it's called off-chip memory also, it's the HPM. It's off-chip because it's not embedded through the streaming multiprocessors. It's separate. And in fact we might have GPUs with different amounts of HPM. And it can be accessed by all the computing elements running on the streaming multiprocessors and also by DOS. Remember that this is our point of contact between the GPU and the host. Last element are caches. We have the global L2 cache, the L1 cache that is private for each stream multiprocessor. We have a texture cache, and then we have shared memory. We will see what shared memory is later. Okay, I will skip this one, I'll go back to that later. So, in the CUDA memory model, which are the components instead? Okay, there is a big difference. Because the CUDA memory model, what it's actually doing is taking all the memories that we have on the GPU and dividing, it divides them in different portions, calling them with different names, and allows the programmer to program these memories. So, firstly, we have registers. Registers, fastest form of memory, it is what is being used for local data storage. So, for example, if a thread declares a local variable, for example, the integer that we use to index the data, where is it stored? Registers. Local small variables are all put into registers. The problem is that sometimes we might require more registers than those we have available. What happens here is a phenomenon called register spilling. Register spilling means, OK, I do not have enough space to store my data into registers. I need to put those data somewhere. I put this information into local memory. Local memory is a type of memory that stores thread local data and can be accessed only by specific threads. I mean, if I have data in local memory, I can access that data. Otherwise, I cannot see what's in local memory. And this is done by the compiler. The compiler puts data into local memory because it's not able to schedule the computation and the data on the registers we have because they are not enough. The problem is that local memory resides in device memory. I have to... I noticed that is an error. I have to fix that. It resides in device memory. So it's on the off-chip memory. If I need to access... Imagine that I need to access every time an index, for example the one that I use to index the data, and every time I need to go to the off-chip memory, get the result and then use it. Because maybe because of a cache miss I cannot read it from the cache. I need to go all the way to the off-chip memory and then get the data back. This is a costly operation, okay? Remember, device memory is off-chip. This is why register spilling must be minimized. Sometimes the compiler is able to do some kind of optimization. For example, if we have some constant variables, it is able to do some tricks and avoid using registers. Minimize register usage is fundamental in order to prevent registers building. So maybe we notice that our application is particularly slow. We might want to check when compiling the application if the compiler is putting data in local memory, there is a flag for that. We can check that because maybe this is part of the delay that is introduced into the computation. Other components, so these two are per thread quality, only threads can access that type of memory and they have their own reserved portion, let's say. Going at a higher level, we have shared memory. What is shared memory? Share memory is a very fast scratch card memory that we have that resides into the L1 cache. So it is a portion of the L1 cache that we have for each streaming multiprocessor. And the nice thing about this is that we can program depth. We can tune the size of this memory location that is dedicated to each block. We can entune it at compile time, at run time. We can decide what goes into that memory. And it is a cache, so it's very close to the computation. The problem, as I said before, is that it is a fast resource because it's close to where the computation happens, it's close to the course. This means it is scarce, usually, and this is one of the limiting factors that we have when scheduling blocks on single multiprocessors. the amount of shared memory per stream in multiprocessor is fixed, then blocks requiring a lot of shared memory are preventing others to be scheduled together with the same stream in multiprocessor. Okay? Yes? Why threads don't use shared memory instead of program memory to spill out from the registers? I think that, okay, I don't know. I don't know. I'll check that and I'll answer you next time. I don't know why this, maybe this is a design choice they made, because this you can program that, the shared memory can program that. So you may want to leave the programmer to be able to entune other resources. Because spilling is not that frequent, you have to use a large amount of resources, of registers for spilling to happen. If you are declaring a local array, so I have a block and I need to declare an array. That array is usually stored in registers, because it's local to the block, to the thread. So it must be pretty large in order for registers to occur. But sometimes when you need that type of resources is also because you need some kind of... storing intermediate results of the computation that need to be shared among the threads. And for this type of data to be shared across the threads, you are required to use shared memory. Shared memory has a scope that is at the block level, so that allows you to to allow threads to communicate. So maybe this is why they made this type of design choice. I'll check that. It's not a big deal. OK. So remember, two limiting factors for block scheduling, number of registers and amount of shared memory. How do we declare data? Let's say to the compiler, put this into shared memory, we have a qualifier, as we had for the kernel, we have the __global__device, etc. Then we have here shared. So what do we need to do is to put the qualifier before the declaration. Let's say we want to declare an array and put it in shared memory, we have to put qualifier shared, then integer, array and the size. You can allocate data in two ways, as I said before, statically or dynamically. For static allocation, the shared type, name and size works just fine. Dynamic allocation is a bit different because you cannot use a malloc, a classic malloc into a kernel code. There is a dedicated parameter in the kernel launch. So, when we are launching the kernel, we have the name of the kernel, the less than, less than, less than, blocks and threads, and then the parameter of the kernel. Within the three less than, less than, greater than, greater than, greater than, after we have blocks, threads, then we can put additional elements still within the symbols. The following element, blocks, threads, then we have the size of the dynamic allocation of shared memory in bytes. They decided to do it this way. And then you use the pointer in shared memory as it was like an array declared as it was static. We will see how to declare stuff in the examples. variables are declared in the kernel code or globally, so it depends on what we want to do in our application. All the threads, the scope is related to the block, so all the threads can access the elements within the data that we are declaring in shell memory. The scope is related to the block, but also the lifetime. So when the blocks complete their execution, then also this memory is free. And other blocks can take place the memory if they need that. Example, static allocation, share, type, name, size. Static allocation is done in C, C++. We index the element, each thread is going to access one element of the shared array. assign a value, this is a copy from the global memory to the shell memory, because maybe we want to get faster access to the data that is in device memory, so we copy the data to the shell memory and use it like a cache. After that we need to synchronize, because we need to make sure that all the threads copied their element before proceeding with the computation because I need that, I need the data. I need to be sure that I have everything that I need to proceed with my operations. This goes also for the completion of the operation. Multiple access of threads to the same block, to the shared memory, have to be synchronized explicitly. Remember sometimes that when you get the wrong results is because you missed a sync thread. So sync threads and syncs the different warps but warps are always in sync? Yes. Sync threads is a synchronization point at the block level. Okay. So when I use like A inside B... Yes. In general, when I use something like that, the array isn't copied in another cache that might be faster. So, in this case you are copying the data from the device memory to the shared memory and then I'm expecting that you are not going to use A anymore. You are going to use SV. If I decided not to do this, then I would need A of I at each point in my kernel. So at this point, what is done is that the reading operation goes through all the caches, like normal, L1, L2, L1, and then the data where it's needed, the point where it's needed. Okay? So this is like the reading or write process is the same as you saw in like the computer architecture process. There is a portion of the L1 that is not programmable, you cannot use the entire L1 for shell memory. That is a small portion that is used for these cases to be used as a real cache that you cannot program and is used to cache the data in order to avoid cache misses and avoid the lock to go through all the lines and get the data from the device memory. I'm not sure if I answered your question. Like if it's already being cached, why should I use shared memory? You are not sure if the data is being cached. I mean, cache lines are overwritten because while you need the data, some other kernel already read something else, or some other block read something else, and then went through all the cache lines, all the cache writes and reads to get the same data, other data. So what you need might not be in a cache that is not programmed. You are not sure to have what you need close by. So SV is going to act as your cache and you will use SV in this case. Otherwise you have to go through the classic process of cache lines and writes. The other type of allocation that we have is the dynamic allocation. We have a pointer, the shared qualifier, extend is a keyword that says OK. The parameter that would go here, you can find it outside. As I told you before, grid size, block size, amount of shared memory to allocate for each block. This is the size that you cannot put here because n is not determined at compile time, but maybe it can be computed at runtime. Maybe you have a matrix that changes the size, you have different pictures, you want to process them in different ways, and you want to adjust the amount of shared memory depending also on the grid and block size. This does not allow you to declare, for example, the shared memory at compile time, because if the images changes their size you don't want to recompile the application every time. So what you can do is to put this additional parameter it's missing a symbol. So what you need is an additional parameter that says okay you have this byte that you have to use in shared memory. Yes, this is shared with all the blocks. All the blocks will have this amount of shared memory allocated. Remember, single program, multiple data. Single program, the kernel is one, all the threads at this same program. What do we use share memory for? Parallel load of data from global memory to share memory. We do not want to go through all the caches when we want to retrieve data from the global memory device memory. Synchronization of block threads we have some kind of intermediate results that we need to reuse, then we can use ShareMemory for that purpose. Save the results there, synchronize and then go on with the computation. Elaboration of block threads on ShareMemory data to store intermediate results, for example. For example, in the Instagram, we can divide the text across the blocks, then each block can compute its own Instagram, then we aggregate the results together, for example. So we need to save intermediate results, and we can do that using shell memory, without committing the results every time to the global memory. of block threads, two times because it's important, parallel store of data from shell memory to global memory, the other way around, the load, but also the store. Remember, access to global memory is costly. We have 32 gigs of the batch memory, but we require a certain amount of of clock cycle in order to get to the data that we want. Yes? I don't understand why we synchronize after storing the data into the cache. Okay. I'm thinking of an example that I can use to to help you better understand the thing. I'm thinking. So imagine that for the histogram, okay, we want to, for example, we are mapping the threads to the letters of the text that we want to process, okay, and we have the block private histogram in shell memory. Okay. Imagine that we have 32 letters. For the sake of simplicity. Each thread is going to update the counter in shell memory. Another one is going to update its own counter that is still in shell memory. To get the correct results, we need to synchronize because the trials might not take the same amount of time to do the same operation. This is why. Sometimes I'm reading, for example, the letter that I'm analyzing. I'm not reading from the L1, but I need to go to the global memory and get the data. This means that two threads, one having the data in the cache and the other one that is in global memory, are taking a different amount of time to do the same operation. of. Okay? So I need to synchronize and then I can go on with my stuff. Because maybe so when we load data into the shared memory maybe there's no space so the data is automatically memorized in the global memory and when the process needs, when the thread needs to retrieve it, it takes more time than the other threads. If it's in shared memory then it's persisting in shared memory and then the thread will go to shared memory. I'm not sure if I got your question. I mean, if there's no space for one thread to save the data in the shared memory so the data is... You are sure that you have space in shared memory. You are explicitly allocating the shared memory every time. It's only the spilling that is kind of beyond your control. We are always sure that you have that amount of shell memory allocated, because it's done at compile time. If it's not enough, then either the compiler or at runtime you will get an error. You cannot launch an application that has too much shell memory for each block. If you are not using shell memory, then I think you will have the entire cache free and you can use it. It's being used as a cache. memory, finally we got to global memory. What is global memory? It is the programmable portion of the device memory that we can use for data exchange with the host. It can be accessed by all the threads across all the blocks on the GPU. Largest capacity, but it's lower, resizing the device memory, HBM or DDR, depends on what you have. The access pattern to global memory must be tuned because we have transactions, so when we read the data from the global memory, we are not reading a single element. If I want an integer, I'm not reading those 32 bits. Instead, I'm reading 32 bytes every time. So this means that besides those 32 bits that I need, those 4 bytes, the others are all wasted. So I need to find ways in order to optimize the access and be sure that I'm using most of the bytes that I'm transferring. This means that I'm having a coalesced access. This is alineato in Italian. I'm giving you a bit of time to think about that. Every time I'm reading something from global memory, I'm not just transferring a few bytes, but a lot of data. I need to exploit the entire data that I'm reading in order not to waste time waiting for data. It's kind of a way to make up for the long waiting time. I'm waiting a lot of time to access the data, but at least I'm not getting one byte. Okay, I'm getting 32. I don't need the other bytes. I'm still waiting, right? Yes. I still have to wait. Yes. Also consider that you still need to wait. But you are always reading those 32 bytes. You need only one, for example, two, three, four. But all the others are going to be written into the cache as well as the others that you need. next one is going to need another 32 bytes in another point of global memory that is going to be written over what you just read. So also the cache is going to be rewritten very often. Finding a nice pattern to read the data from global memory is going to help your application most of the time. Sometimes you cannot just do that. I need to find a configuration of the data that I'm copying or retrieving from the GPU that is nice enough for me to write stuff together or to read stuff that is being used by multiple times. can put... This is why we might want to put data in a column-major fashion. So they are closer? So they are closer, maybe I'm processing columns instead of rows, and maybe I want to put in this way, putting in a column-major is going to help the fact that I'm reading in chunks. So when I read the first I already have the cache, the others? Yes, exactly. Another type of memory that still resides in device memory is the constant memory. Constant memory is read-only for the GPU. Of course, the OS can write this type of memory, otherwise it's useless on the GPU. There is an API dedicated to constant memory usage, that is, APIs with symbols. So for example, we want to copy data into constant memory, we have memcpy to symbol. We will not see how to use constant memory because it's a bit weird. It's used to store data that remains constant throughout the computation. And the optimal access pattern to this type of memory is when all the threads are actually reading the same location. So the value is broadcasted to all the threads. For example, as before, when we were multiplying the matrix by 2, it's a very simple example, but we could have put that 2 into constant memory. That value is going to be broadcasted to all the threads that are using the same value at the same time, I don't know, in this way we have an optimal access pattern and all the tracks are using the same data that is broadcasted from the constant memory. For example, if it works with one integer, imagine when we have more complex algorithms, when we might have more complex data or larger data that is broadcasted to all the tracks. at the same time. This is a very nice pattern. It's nice to have. Sometimes you don't use it, because not every application requires constant memory. But having this pattern sometimes benefits the computation. Last component, texture memory. Device-wide read-only. It still resides in the device memory. And it is optimized for spatial locality and in particular 2D locality. Let's say it dies for storing matrices that are broadcasted, kind of. Optimized for what? I'm not sure it's broadcasted actually. But it's optimized to handle matrices that are constant throughout the computation. It's called texture because it is used to store textures for graphics applications. Textures in video games are stored in texture memory. The last thing I wanted you to see is the slide that I skipped before. Okay. That are the latencies that we have when we access the memory. Registers. On the streaming multiprocessor, close to the thread, very few plot cycles. Share memory. Closer, close enough to the threads, almost one clock cycle. L1, one clock cycle, is a cache, we use that as a fallback, let's say. Constant cache, another nice pattern, one clock cycle. Read-only memories in general have one clock cycle. For the device memory, we are paying the fact that we have gigabytes of available memory. We are paying that having at least 200 clock cycles to access the data. This is mitigated with the fact that we are transferring more data, so it is kind of amortized. But this also means that we have to optimize and make a very smart use of that bandwidth that we have. So those bytes that we transfer, we have to exploit them efficiently in order not to pay 1000 cycles every time we want something from probable memory. Key takeaway from this slide, in general, make the lowest amount of reads possible from global memory. If you can store something in shared memory, good. Otherwise try to optimize all the access that you have to global memory. Then you pay the fast access with the amount of resources that you have. You are not given registers for this. I think this is all on my side. I finished a bit earlier than last time. I hope everything is alright. If you do have questions, feel free to ask now or via email and we'll try to get back to you as soon as possible. Have a good evening and see you on Thursday.