 You can watch it later. So, okay, so we're 1st start, uh, we will start this lecture with our 1st, uh, ends on exercise. So we are going to write some code today. Um, and we are doing, uh, how we can, uh, parallelize the computation of an Instagram. So, let's see some, I would say theory before. So, what's an Instagram? It's a display of the frequency of data items. So, uh, what we are doing, we are counting, uh, stuff. And we have, for example, we are counting characters. We are counting pixels of an image and we want to determine the number of the, um, of occurrences that we have for a given character for a given pixel. Uh, they are used, uh, for example, to summarize data sets and that are used for feature extraction. Okay. Um, okay. So, how does the competition of an Instagram work? Uh, you already have the code online. I put it, uh, online today. So if you want to take a look at the code at the sequel. You can, um, yeah, very done is simple is sequential. Okay. So, uh, we, we have a function, for example, in this case. That computes the Instagram, uh, we are counting characters from, uh, string a very long string. So, when we have some. And we want to. Give the user back the, the Instagram. So the count of the occurrences. And we have the length of the of the text that we are analyzing. Okay. So, for each character. We shift the data that we have to make it an integer. Okay, so we subtract the a character. Then we check if what we get is a valid value. So maybe in ASCII, we are going beyond the. The ones, the characters that we can take into account. Then, if it's a valid character, we are, we are increasing the counter. For the corresponding bin, so for for simplicity, today we are like grouping characters. into uh into us fixed number of beans so we are not going to have a counter for each letter but we are doing like for example from a to e uh they are they all increase the same counter and then we're going to two blocks okay so the computation of the instagram is simple and efficient uh it's very fast on the cpu because everything fits well in the level one cache of the of the cpu so it's a it's a small computation it's sequential so the cpu is going to take a very short amount of time to compute the histogram okay um everything is accessed sequentially the cache lines are well used so we expect the computation to be memory bound okay so we are bounded by the amount of data that we can transfer from the gpu from well from the hvm or from the dram to the streaming multiprocessor so where we have the computation let's see how we can uh compute an instagram with the gpu so i'll stop sharing Oh, okay. I stopped the wrong. Okay. Okay, okay. So, here we have the. The sequel, right so we have our main function. Uh, that gives, uh, gets as input. The, um, the text file that we are using. Can you see the, the code? Okay. And then, uh, it reads, uh, the lines and then computes the Instagram. And we have all the beans. Okay. So we are going to have, uh, 5 beans in which we are grouping the letters. The 26 letters. Okay. The computation of the Instagram takes place as before. Okay, as I told you before, so we have. We shift the data and then we count if it's a very characters, then at that point, we. Increase the counter for the in the corresponding mean. Okay, so if we are given a C. code, how we can transform it to GPU1. Okay. So let's do it like step by step. So first thing we have to rename the file. So as we told before, the other lectures, we need the CU files, not C files, not CPP. Okay. For CUDA requires CU files. Okay? So what we need here is a function that does the same thing at least on the GPU. Okay? This is the first step that we need. So, what we need is. Another function. Okay. That we call that we can call GPU Instagram. Or simply Instagram. Okay. And this is going to be our GPU function. GPU kernel, let's call it with. With the correct name. Okay, this is going to be our GPU. How do we tell the compiler? This is the stuff that we are going to compute on the on the. We need to put before the qualifier. That tells us this is the entry point for our current. That is the. Okay. Sorry, um. Papa. Okay, sorry guys. I. The global qualifier, this is the qualifier that tells us. Okay, this is the function that is going to be involved. By the application to be executed on the. Okay, we know that we have a buffer. With the data, so with all the characters. We have a buffer with the, um, with the beans of the Instagram. And then the length that tells us, okay. Um, this is the length of the text that we are analyzing. Okay, we have our kernel. And then we need to invoke it. On the outside. Okay. So. Which are the, which are the things that we need. To properly invoke the. We need the kernel invocation itself. That would be something like histogram. With the weird symbols. Okay, then we need something to put as the arguments. Okay, and then. We can invoke it also. We need to allocate the proper memory. And to transfer the data, okay, otherwise we are not going to be able to. To execute stuff on the GPU, because the data is not on the GPU. Okay, so. Just to be sure that I'm not making any mistake. Uh, so. The 1st thing is going to be. The text. Okay, so we need. A buffer dedicated to the text. As I said, the other times we need the. Pointers that are dedicated to the memory spaces that we have on the. So, we are going to have a chart. Star, uh, text. I use underscore D to identify those pointers. That I can only use associated with the APIs or kernel invocations. Okay, then I also need the. A pointer for the histogram, right? Yeah, the input for the function is the histogram. Then I need a pointer for the histogram. Let's call it this way. Then the length is the same, is passed through a copy, as a copy. So I don't need to have a buffer allocated for that. Okay, so as I said the other times, we need buffers only for like array, matrices, and multidimensional data for variables, unless they have the results, okay, we can copy them from the GPU to the GPU. Okay, we have the pointers. We need to allocate the memory for the structures. for the structures. So let's call a CUDA malloc. Okay. We have our CUDA malloc. Let's do it for the text. So we need the address of the text. Well, we need the variable for the pointer of the text. How much space do we have to allocate for the text is the same size that we have here, right? Are you okay with that? We have the same space on the CPU. We are going to have the same space on the GPU. Perfect then we are going to do the same thing for. The histogram. How much space. Do we need we need the same space we have on this that we allocated on the. The CPU, so this is going to be 5 times. Size of. Okay. Are we good? Okay. We have allocated the space. On the on the device, we need to move it to move the data. Remember everything must be done explicitly. Okay. Did you feel is not going to take that take care for that. For you, you have to move data explicitly. Every time. Okay. So, at this point, we can invoke a copy. And copy, which is the syntax for the good amount of copy. We have the destination. The source, the amount of data that we want to transfer. And then the direction. So we have, we want to move data. Who the text on the device. From the text on the CPU. And we want to move an amount of data that is equal to. The size of the chart. Okay. Times the length of the, of the sequence that we are analyzing. Okay. So this is going to be land times. Size of. Last argument. Direction that is. Who the copy. Device. Are we good? Okay. So, so far we have. I look at the space, so if we wanted to be like, precise. We would need to do the same thing for the histogram. Or at least set the memory to 0. On the. Just because we are not doing this, here we are incrementing a counter. We need to make sure that it is zero. So to be safe, let's do it this way, just to be safe. There is another function that allows you another API that allows you to. Set memory to a certain value that is. Okay, we could do that that way, but for now, let's do it with a copy. Okay, okay. We have set our, our current parameters parameters. We need to find the configuration of the current. Because we need to put something into the. Into here. Okay. So, 1st, let's place. Text the, and. Is the. Land in here we need to find the parameters. The kernel, so let's say. That, um, we want, uh, uh, we said that every time we need to have a block that is, uh, size is, um, um, a multiple of 32. Okay, let's say that we have a block that is, um, that has 32 threads. Okay, a multiple of 32 just works. Okay. I just put a define there to make it easy. We said that the grid is multidimensional and we have a built-in structure that is the DIMM-free data structure that allows us to define the grid. So, we have the threads per block that is going to be Block them 1 and 1. We want it with our single dimension. Okay, we are analyzing an array. Mono dimensional. Okay. So we want. 1 single 1 of the national green. Okay. Okay, then we have to do the same thing to, uh. Determine the block the grid size. Okay. Great. Okay. What do we need to do? We can make sure that our kernel is actually dividing evenly, or at least tries to divide evenly, the load across all the blocks. So, one thing we can do is split the text through the blocks, and then each thread is going to take care of a portion of the text. Okay, so what we can do is divide. Is set the number of, um. Of blocks that is equal to. Land plus block. Minus 1 divided by. 1 and 1. So this means that we are, um. Rounding up. The size of the, um. Of the text in the case is not a multiple of, uh, of our block size. Then we divided across the blocks. In this way, we can determine the number of blocks that we need, depending. On the size of the block that we just decided. Okay, and we can put the parameters. In here. Are we good? Okay, then. We have our current location, we copied the data. We launched the kernel, the GPU starts the competition. Then what do we need to do? Wait for the GPU to complete the computation. All right. Device synchronize. This allows us to wait. It is a blocking function that allows us to wait for the GPU to complete the computation and ensures us that until we reach the point in the execution, the GPU has completed. We are sure that the GPU has finished. What do we need to do now is to get the results from the GPU. What do we need to copy is the histogram in the other direction. So we copy the data from the GPU to the CPU, the same size. The other direction, so it's going to be. Then copy device. Post. Let me just copy. This. snippet here okay sorry let's see this way okay i'm just changing this one here The last thing I would need to do is to free the memory and check the results. So I'm copying just the for loop that checks the result, okay? And I need the Instagram hardware. Okay. So what I did is instead of copying the data and putting it back into histogram, I created another variable that is histogram hardware to which I can copy the results. Then I have the histogram that is computed on the CPU. Then I have the histogram computed on the gpu saved into two distinct histograms and then i can compare the results and then i can check if everything i did is correct okay so let's see if it's going to compile so the invocation of the kernel well the compilation of the kernel is the same as a C application. Okay, I have the name of the compiler, that is MVC, the name of the file, minus O, the name of the application, the executable. Let's call it isto, one error. Okay, let's call it this way. Okay. The error was due to the fact that I had a variable and the function with the same name. So. The compiler did not like that. Okay, I have my application. We can run it and try to see. What's coming out of it? I have a million character. Without spaces. Okay. Uh, generously. Help me with that. There's something wrong with that. Okay. Let me see what I did wrong. Maybe I did not put the put that on time. I don't think so. Looks. I put the defined before we should change something that is not correct. Please tell me. We should change the function. Okay. Let me check. Hmm. Let me see. Uh, okay. I got it we need to do something on the cabinet. Okay, because we just copy the paste it. Okay. So, we need to make sure that okay. I was missing the part. We need to make sure that the, um, the actually does what we are. Expecting okay. So, we said that we needed, um, the, um, the. Yes. Um, to compute, um, the. The stuff that we need. Okay, so I'll just switch to the. Code that is already done. Okay. And we are going to see the other stuff later. So, what do we need to do is to identify 1st. The thread, so that we can map it to the corresponding character. We have our thread. We have block index and block dimension in order to. Switch and to increase the counter. Okay. Because the first thread in the second block is not supposed to analyze the character in position zero. That is actually the thread index, okay? But must analyze the character that is in position 33. Okay, so we need to do the same thing that we also saw the last time and the time before to shift the index, okay? So we have our thread that analyzes the character I, okay? We retrieve the character I, we shift the character with the corresponding A character, okay, this first char is A, okay? And if it's valid, then we increase the counter. Let's try this way. This is 1. Okay, minus all. Compiles let's try is to. Okay. We have a problem. The is not going is not like. Doing the computation correctly. We have all the beings that are going to give us. Different results and wrong results. Why do we have this. Phenomenon well, the thing is. It's thread is doing the same. It's only instruction. Okay. It's performing this instruction independently from the other. Okay, in parallel, so what actually happens here. Okay, is that. We are incrementing a counter that is not updated. So, I read all the threads, read the same value from the bin that is a zero, for example, okay, at the first round. Then they all increment it together by 1. So, if they're doing the same instruction at the same time, they are actually incrementing only by 1, the counter. Not by 1 times the thread that are accessing the bin. So what happens here is that we would want this um configuration so we read the value we increment and then we write the bin and then the second thread that does the same thing the problem is that is the gpus does not ensure that this happens all the threads read the value they do their own stuff and then they write back But what they write is not updated because they wrote an old value. The histogram is sequential. Okay. The problem here is that we read the value for the 1st thread, for example. We update our own counter. Meanwhile, the other thread reads the same old value that I read. So I'm not updating the correct counter. And not updating the correct number. I'm not incrementing the counter correctly. Okay, we have the support the race conditions. We are going all to get the, the, the same value that is not updated. And we have incorrect results. How do we solve this problem? Fortunately. There is an instruction that. It's exposed to the programmer that are atomic instructions. Atomic instruction are instructions that perform mathematical operation. That are single inter uninterruptible operations. So, this means that once I invoke. The atomic instruction on a thread. On threads. Okay. They are actually serialized. So, the 1st thread, they are executed in order the 1st thread. Reads and increments the counter, for example, in our case. Then we have the 2nd thread that does the same operation. But the value is updated, so we can get the correct results. Okay, we get back to this configuration. That is the correct 1 instead of this 1. Because the operation is uninterrupted. Okay. We do not have any race condition anymore. Can the instructions can be used on both global and shared memory. So, this is great. There are a bunch of instructions that we can support through atomic instructions. So we can do addition subtraction swaps, minimum, maximum, depending on what we need in our application. Let's see, how is it done? Okay, let's go back here. This is the stuff that I. Okay. We need to do instead of increasing the counter. Like this, like we would on a C C like program. We need to call an atomic instruction. This allows us to. Increase the counter. By 1, all the operations. Are serialized, so let's see. Okay, in this case, we do not have any error. Everything's good. Okay. Perfect. The problem is that. All the operations are serialized. So it's the same thing that. As doing it on on a CPU, right? Plus. The other time that we need to transfer the data back and forth. So this is not very efficient. Okay, I do not have the timers here on the CPU version on on the GPU. But, I mean. It's serial, so we are not actually taking advantage of the parallelism that we have available on the GPU. All the threats are are computing the same instruction 1 after the other because of the atomic operation. Okay, anybody has any idea. On how we can parallelize this, uh. This step. Maybe, uh, today. Increments all the value, so we can compute if we do have to be. And then synchronize the press and. Just 1 for each that has completed that. We should okay so. You say, um. So you're saying. Each thread computes his own stuff. And then I. Increment the. The global counter, let's say the global. Okay, that might be a way to do that. But we can do like another optimization. It should be. That might even require even more computation. Because you are mapping beans. Like, um, each thread has to scan multiple times the text. If I understood correctly what you said, so this is not like. Super efficient, we said that we mapped well, you are on the right track. Okay. Same. I can kind of. Divide the text. Through the threats, and then all the tries compute their own Instagram and then committed the result. This is the kind of idea. On the 1st optimization that we can done instead of mapping it to threats. We can do it by blocks. Makes sense. Okay, so we map. Even also considering that we have, or we already have the, uh, the string. Spread through across the, the blocks. Okay, so what we, what can we do is. Um, each block is going to. Compute his own Instagram and then we commit on the results. It's an upper level. Also, using the block mapping allows us to take advantage of shared memory so that we can reduce All day data transfers from. They like thread local variables to the global memory, because this is another problem that we have here. We have a huge traffic to global memory. And remember, global memory. Is abundant, but it is scarce. Well, it's abundant, but it's low. Okay. So, instead of spending. 200 300 i don't know how many cycles to get to the to the counter and then get back and then update it we are paying them in full okay we can compute the local histogram and then commit the result back to the global memory So, let's see how this code changes with the let's call it privatization. Okay. A 1st, the privatization step that we can do. Is actually okay. I wanted to compare them. This way. Okay. Perfect. Our 1st privatization step we can do is. Like, put the. The Instagram well, each block is going to compute its own Instagram. Okay. But the Instagram is in global memory. Okay, let's do it step by step. So. How does the code changes? Yeah. In the on the outside we need to. Allocate the memory for all the histograms also. Okay, we allocate the memory for the histograms. We launch the kernel and then we copy back. A single Instagram, what we are doing is. I compute all the Instagram, then I sum them. Into 1 and I, and I copy back all the, the 1 with the, the cumulative results. So, what happens here is I have my thread identifier, I have the stride that is the shift that I have in analyzing characters. Each thread is going to analyze a character. Well, more characters separated by stride characters. And I'm going to, let's do it this way. Okay. I'm performing the atomic add through my own histogram that is at this, starts at this point in global memory. So, we have all the contiguous histograms 1 for each block. How do I identify. The histogram that corresponds to my block it's. Block index times the number of pieces that I have in my Instagram is the same thing that we did for the. For the characters in, uh, in the text. That we are analyzing. Okay, then this is the same. Operation that we did before. At this point, I have this instruction that synchronizes the threads. If I'm block zero, I'm going to be the one that has all the results. I'm going to accumulate all the results of the histograms into my histogram. Okay? Okay. So I'm going to want to, that is going to retrieve the results from the other. Okay. This means that this operation is going to be performed only by, um. All the blocks that has that have the, um, any index that is. Higher than 0, so if I'm not 0, I'm going just to wait. For the others, my computation is done and. I'm just going to, like, do nothing. If I'm not block 0. I have to wait for all the threads to complete the, the computation. 1st, then. What I'm doing is to. Take the value. And if it's higher than zero. I'm going to put it in the, in the, in the Instagram. Of the of the block 0. Okay, this allows us to have a 1st, um, optimization. We can do better. Okay. We will see how we can do that, but this is the 1st optimization. Okay. This, these are all means. Test to the compiler to like. Explicitly write all the instructions in the loop. Okay, so that I can do them. 1 next to the other I, I do not actually have a loop. It's, uh, it's called a rolling like, I have all the instructional or from the loop that are sequential. 1 after the other instead of explicitly writing them, that would be the same. I put a pragma role. Okay. Then it's the value is higher than 0. I'm going to update this. Okay. Let's see. This is D4, because I just want you to see a thing. Okay. Let's see how much time the kernel takes. Okay. in analyzing the text that I send, okay? I'm going to use inside systems to profile the application, okay? So this means inside systems, profile, I want all the statistics, then invoke my program with the argument. Okay, everything's correct, so perfect. I have a bunch of data that the reports gives me, okay? So I can see which are the instructions that require the most. I have the time of all my CUDA APIs. Okay. And I also have the sum of all the kernels invocation. So, in this case, I have 1 kernel invocation. That is histogram kernel. Okay. That takes. 27,000 nanoseconds. Okay. The time is short. Because I'm not giving the GPU a very large text to analyze. Okay. Let's see how does the, how this parameter changes with the optimization that we are doing. Okay, I also have here, uh, the, um. The provider also gives the time required by memory. transfers in both directions because I have one direction and the other, and also the amount of data that I'm transferring. Okay. So in this case, I have a nice report of all the main metrics that I might be interested into, and it gives a nice quick view of how your application is doing. So I suggest you use it and you see how changing the kernel impacts your performance, specifically on the kernel time. And also looking at the memory is also important because you might want to keep an eye on the time required by data transfers. So, sometimes you are transferring too much data, or you are transferring, like, a bunch of data that is not actually needed. So, this might help you. Take a look at that and also to. Like, um, notice that you are doing some something strange. Okay. Okay, now that we have. Okay. Now that we have our, um. Application written with the private Instagrams. We can move this stuff into short memory. So, for. I'm going to use them already written because I'm sure I'm going to make some mistake. Let's put it 1 next to the other so that we can see how it changes. So, the current location is the same the threads. How do we identify the trends at the same? Because we are analyzing the same stuff. We are only putting the results of the computation in another place. Okay. We have our own private bees that are. In short memory, so is our Instagram local to the block. And we have the qualifier shared at the beginning. It tells us it's a variable. That is going to be put into shared memory. This is nice. Quick and clean, so we have a specific qualifier that. Does the does the job there's a compiler to do the job. This is the operation. Okay, um, just okay. I wanted to do to put it. Okay. First thing 1st, we need to initialize. Our histogram to 0. Because we allocated it, and then we need to make sure that we are not. Incrementing random numbers, but actually what we want to see is the, are the frequencies. So we need to make sure that the variable is initialized. We do it like. In parallel, okay, we have each thread that is updating. an element of East OS, and this loop is unrolled, so everything is done in parallel. And then since we are operating with shared memory, we need to synchronize the thread because the threads might take different, a different number of clock cycle doing the same operation. Okay? Or maybe if we have more than 32 threads, maybe we have 64, we have two warps. Warps are not working well. Threads within the warp, yes, are working in lockstep, but two warps are not synchronized, so we need to explicitly synchronize the threads. Let's put a barrier in here, and then we have the same operations that we had before. Right? So, this is the same look to count the currencies. But instead of writing it into global memory. We have our own local operation that is very close to what we did at the beginning. Okay. And then we have to commit the result to grab to global memory. In this case, we do not all the blocks. Have to do the same operation, because all the blocks have their own histogram that is local that is on a private portion of memory. And all the blocks have to commit the result. To the global memory, so in this way. We have a much less. We have much less traffic from the streaming multi processors to global memory. Because we have in this case, since the block, the bins are 6. For each, for each block. Then we are going to have each, for each block, 6 active threads. At maximum, they are going to perform an atomic add. Only if necessary. Okay. So, if, for example, a block only finds characters that belong to the first bin, then what we are doing is only the first thread is going to update the corresponding value. And this is way less traffic to global memory. So, way less latency and way less serialization of operation across all the GPU. Okay? Is everything okay? Yes, yes. Yes, it's changing size. And we have, in fact, a malloc that tells us to. I locate an Instagram on the device. That is that has these. Amount of bytes, let's say, okay, so this is been numbers times the size of assignment. So this is going to be. A single Instagram. Okay. In this case, we have only these, uh. Uh, our single Instagram in the global memory. Because each each 1, each block is going to have. It's own, uh, Instagram local in short memory. Okay, let's see. So, let's see some performance. Remember that is 27,000. Okay. So, let's compile. This is 5. Okay, and let's compute. So, okay, I wanted to check if the results were correct. Okay. Then let's see, I have the same list of operations. Using share memory. Greatly the crisis the. The time required to compute the Instagram. Okay, we have 3 times less. We, this solution takes. 3 times less less time. Okay. I'm not able to say that, but 3 X over the previous version. Okay. And if you wanted to compare the amount of memory that you are transferring. Papa, I saw this is. Okay, it's pretty much the same. It's pretty much the same because the, um. I'm sorry. Okay, the version before was already the one with the. Was copying back 1 Instagram, so this is why the. I'm out of memory back. Was solo. Okay. It didn't change much because we were already. Um, already in transferring the same amount of data that is like. 20 bytes. Okay. So, let me see. If I have something else. Okay, okay. So, this is going to these are going to be the last 2 slides. Then I'm done for today. Uh, so. Take home messages from these, uh, from this optimization that we saw. I'm going to give you all the codes so that you can like, look at them. Okay. Uh. A lot of latency that we have from accessing data to the more, uh, or whatever type of global memory you have on your. Continuously interacting with the data. Is going to lower your performance. So you put up a private histogram for each block. Again, also, in this case, you need to use atomic addition atomic operations. Because of the fact that the histogram. Is a sequential operation. Okay, so. Every time you need to make sure that you are updating. The correct value. Okay. Then what you can do is commit to. The data, the, the global memory. Only the value that you are actually. Required to to use on the outside. We are all interested in the final result. So, firstly, from the, we only copy that the portion that we need. And then in the global memory, we only put the data that we, that we are required to transfer. Okay. So we commit the results and we try to interact. As, um, as few times as possible. Okay, so that we do not have to wait. all the times for the data to get back to the global memory. This is all on my side. I've been very quick today. But this was like a first introduction to the CUDA programming model. This was a very simple example. And I was not expecting to take like two hours today. But it's, um, it was important to see. How different optimization, uh, can impact the performance, uh, 1st glance at the compiler 1st glance at the profile. And also see how, uh, optimization can really impact the performance. Okay. Very simple example, but I believe it's 1 of the meaningful ones. Okay. If you have any questions, I'm here, I'm available. I finished early, so happy to ask answer all your questions. Um. As usual, I'm going to take like, 1 or 2 days to upload the, the lecture. And also, I'm going to upload the code. Um. I will try for the next time as someone asked me last time to change the room. so that you are more comfortable. I'm not ensuring anything, OK? If they are available, I will change it. OK? Thank you.